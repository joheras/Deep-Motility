# -*- coding: utf-8 -*-
"""Copia de DeepLabV3+.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yIo6CJ67XQwnDDOXXpxv91Ak6i1SMryO
"""

from fastai.basics import *
from fastai.vision import models
from fastai.vision.all import *
from fastai.metrics import *
from fastai.data.all import *
from fastai.callback import *

# SemTorch
from semtorch import get_segmentation_learner

from pathlib import Path
import random

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import numpy as np
import torch
torch.cuda.set_device(0)

"""Function por stablishing the seed for reproducibility"""

number_of_the_seed = 2020

random.seed(number_of_the_seed)
set_seed(number_of_the_seed)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

path = Path("dataset/")
path_images = path/"Images"
path_labels = path/"Labels"

test_name = "test"

"""Obtaining the label associated with an image, in the case of segmentation it is a mask"""

get_y_fn = lambda x: Path(str(x).replace("Images","Labels").replace(".jpg",".png"))

"""Aux function for splitting between training and testing"""

def ParentSplitter(x):
    #print(Path(x).parent.name)
    #return str(x).replace("train","test")
    return Path(x).parent.name==test_name

"""Loading the codes of the different classes. All the datasets have the same codes."""

codes = np.loadtxt('dataset/codes.txt', dtype=str)

name2id = {v:k for k,v in enumerate(codes)}
print(name2id)
void_code = name2id['Bacteria']

def tumour(input, target):
    target = target.squeeze(1)
    mask = target != void_code
    return (input.argmax(dim=1)[mask]==target[mask]).float().mean()

"""Early Stopping parameters"""

monitor_training="valid_loss"
comp_training=np.less

monitor_evaluating="dice"
comp_evaluating=np.greater

patience=3

from albumentations import (
    Compose,
    OneOf,
    ElasticTransform,
    GridDistortion,
    OpticalDistortion,
    Flip,
    VerticalFlip,
    Rotate,
    Transpose,
    CLAHE,
    ShiftScaleRotate
)

class SegmentationAlbumentationsTransform(ItemTransform):
    split_idx = 0
    def __init__(self, aug): 
        self.aug = aug
    def encodes(self, x):
        img,mask = x
        aug = self.aug(image=np.array(img), mask=np.array(mask))
        return PILImage.create(aug["image"]), PILMask.create(aug["mask"])
    
transformPipeline=Compose([
                        Flip(p=0.5),
                        VerticalFlip(p=0.5),
                        Transpose(p=0.5),
                        Rotate(p=0.40,limit=10)
                    ],p=1)

transformPipeline=SegmentationAlbumentationsTransform(transformPipeline)

class TargetMaskConvertTransform(ItemTransform):
    def __init__(self): 
        pass
    def encodes(self, x):
        img,mask = x
        
        #Convert to array
        mask = np.array(mask)
        
        # Change 255 for 1
        mask[mask==255]=1
        
        # Back to PILMask
        mask = PILMask.create(mask)
        return img, mask

"""Definition of the splitter for saying the learner what weights of the network to freeze"""

def segmentron_splitter(model):
    return [params(model.backbone), params(model.head)]

"""# DeepLabV3+

## Manual Annotations
"""

manual_name="train"

path_manual_img = path_images
path_manual_lbl = path_labels

fnames_manual = get_image_files(path_manual_img)
lbl_names_manual = get_image_files(path_manual_lbl)

size = 1002
bs = 1

"""### Data Augmentation"""

manual = DataBlock(blocks=(ImageBlock, MaskBlock(codes)),
                   get_items=partial(get_image_files,folders=['train','test']),
                   get_y=get_y_fn,
                   splitter=FuncSplitter(ParentSplitter),
                   item_tfms=[Resize((size,size)), TargetMaskConvertTransform(),transformPipeline],
                   batch_tfms=Normalize.from_stats(*imagenet_stats)
                  )
manual.summary(path_images)
dls = manual.dataloaders(path_images,bs=bs)

"""WD=1e-3"""

fname="unetModel"

callbacksFitBeforeUnfreeze = [
    # ShowGraphCallback(),
    EarlyStoppingCallback(monitor=monitor_training,comp=comp_training, patience=patience),
    SaveModelCallback(monitor=monitor_training,comp=comp_training,every_epoch=False,fname=fname)  
]

learn = get_segmentation_learner(dls=dls, number_classes=2, segmentation_type="Semantic Segmentation",
                                 architecture_name="unet", backbone_name="resnet34", 
                                 metrics=[tumour, Dice(), JaccardCoeff()],wd=1e-3,
                                 pretrained=True, normalize=True,cbs=callbacksFitBeforeUnfreeze).to_fp16()
learn.lr_find() # find learning rate
#learn.recorder 
learn.freeze() # Freezing the backbone
learn.fine_tune(15,freeze_epochs=2)

